{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AppML_InitialProject_train.csv\")\n",
    "real_data = pd.read_csv(\"AppML_InitialProject_test_regression.csv\")\n",
    "\n",
    "target_column = 'p_Truth_Energy'\n",
    "all_features = []\n",
    "for col in df.columns:\n",
    "    if col != target_column and col != \"p_Truth_isElectron\":\n",
    "        all_features.append(col)\n",
    "\n",
    "scaler_fs = StandardScaler()\n",
    "X_normalized = scaler_fs.fit_transform(df[all_features])\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=all_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981857c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = X_normalized\n",
    "y_temp = df[target_column]\n",
    "X_temp_train, X_temp_test, y_temp_train, y_temp_test = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_temp_train, y_temp_train)\n",
    "\n",
    "rf_importance = rf_model.feature_importances_\n",
    "top_rf_indices = np.argsort(rf_importance)[-70:]  # Top 70 features\n",
    "rf_features = [all_features[i] for i in top_rf_indices]\n",
    "print(f\"Features after Random Forest analysis: {len(rf_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32973c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf = X_normalized_df[rf_features]\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    device='cuda',\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_model.fit(X_rf, df[target_column])\n",
    "\n",
    "# Calculate SHAP values using XGBoost\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_rf.iloc[:1000]) \n",
    "shap_importance = np.abs(shap_values).mean(0)\n",
    "\n",
    "# Select top features based on SHAP\n",
    "top_shap_indices = np.argsort(shap_importance)[-20:]\n",
    "selected_columns = [rf_features[i] for i in top_shap_indices]\n",
    "print(f\"Final selected features: {len(selected_columns)}\")\n",
    "print(\"Selected features:\", selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df[selected_columns]\n",
    "y = df[target_column]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "X = np.array(X_scaled)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49a024",
   "metadata": {},
   "source": [
    "# XGBoost solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "dall = xgb.DMatrix(X, label=y)\n",
    "\n",
    "print(\"Performing Hyperparameter Optimization\")\n",
    "\n",
    "#parameter search space\n",
    "param_grid = [\n",
    "    {\n",
    "        'max_depth': np.random.randint(3, 15),\n",
    "        'learning_rate': np.random.uniform(0.01, 0.21),\n",
    "        'subsample': np.random.uniform(0.6, 1.0),\n",
    "        'colsample_bytree': np.random.uniform(0.6, 1.0),\n",
    "        'reg_alpha': np.random.uniform(0, 1),\n",
    "        'reg_lambda': np.random.uniform(0, 1),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'device': 'cuda',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    for _ in range(10)\n",
    "]\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_params = None\n",
    "best_cv_results = None\n",
    "\n",
    "for i, params in enumerate(param_grid):\n",
    "    print(f\"Testing parameter combination {i+1}/10...\")\n",
    "    \n",
    "    # Use XGBoost's native cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        params=params,\n",
    "        dtrain=dall,\n",
    "        num_boost_round=500,\n",
    "        nfold=3,\n",
    "        metrics=['rmse'],\n",
    "        early_stopping_rounds=50,\n",
    "        seed=42,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    final_rmse = cv_results['test-rmse-mean'].iloc[-1]\n",
    "    \n",
    "    if final_rmse < best_rmse:\n",
    "        best_rmse = final_rmse\n",
    "        best_params = params.copy()\n",
    "        best_params['n_estimators'] = len(cv_results)\n",
    "        best_cv_results = cv_results\n",
    "        \n",
    "    print(f\"  RMSE: {final_rmse:.4f}, n_estimators: {len(cv_results)}\")\n",
    "\n",
    "print(f\"\\nBest parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    if param != 'n_estimators':\n",
    "        print(f\"  {param}: {value}\")\n",
    "print(f\"  n_estimators: {best_params['n_estimators']}\")\n",
    "print(f\"Best cross-validation RMSE: {best_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700796d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cv_results = xgb.cv(\n",
    "    params={k: v for k, v in best_params.items() if k != 'n_estimators'},\n",
    "    dtrain=dall,\n",
    "    num_boost_round=best_params['n_estimators'],\n",
    "    nfold=3,\n",
    "    metrics=['rmse', 'mae'],\n",
    "    seed=42,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "final_rmse = final_cv_results['test-rmse-mean'].iloc[-1]\n",
    "final_rmse_std = final_cv_results['test-rmse-std'].iloc[-1]\n",
    "final_mae = final_cv_results['test-mae-mean'].iloc[-1]\n",
    "final_mae_std = final_cv_results['test-mae-std'].iloc[-1]\n",
    "\n",
    "print(\"Cross Validation Results:\")\n",
    "print(f\"RMSE - Mean: {final_rmse:.4f}, Std: {final_rmse_std:.4f}\")\n",
    "print(f\"MAE - Mean: {final_mae:.4f}, Std: {final_mae_std:.4f}\")\n",
    "print(f\"MSE - Mean: {final_rmse**2:.4f}\")\n",
    "\n",
    "print(\"Training final model\")\n",
    "final_model = xgb.train(\n",
    "    params={k: v for k, v in best_params.items() if k != 'n_estimators'},\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=best_params['n_estimators']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b68558",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(dtest)\n",
    "\n",
    "# Predictions on real test data\n",
    "real_data_select = real_data[selected_columns]\n",
    "# Scale the real data using the same scaler\n",
    "real_data_scaled = scaler.transform(real_data_select)\n",
    "\n",
    "dreal = xgb.DMatrix(real_data_scaled)\n",
    "y_pred_XGB = final_model.predict(dreal)\n",
    "\n",
    "# Evaluate the model with regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353411e7",
   "metadata": {},
   "source": [
    "# Neural network solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer with batch normalization\n",
    "model.add(Dense(512, input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "    \n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='huber',  # More robust to outliers than MAE\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=15, \n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=8, \n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training improved model...\")\n",
    "# Train \n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled, \n",
    "    epochs=200, \n",
    "    batch_size=128, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# \n",
    "# Comprehensive evaluation\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'\\n=== Model Performance ===')\n",
    "print(f'Final MAE: {mae:.2f}')\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "print(f'Mean target value: {np.mean(y_test):.2f}')\n",
    "print(f'MAE as % of mean: {(mae/np.mean(y_test)*100):.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58680f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
