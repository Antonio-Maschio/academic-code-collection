{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering, AgglomerativeClustering, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "np.infty = np.inf  # Monkey patch for compatibility\n",
    "\n",
    "import umap\n",
    "import warnings\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringFeatureSelector:\n",
    "    def __init__(self, data, n_features=10):\n",
    "        self.data = data\n",
    "        self.n_features = n_features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaled_data = self.scaler.fit_transform(data)\n",
    "        \n",
    "    def pca_based_selection(self):\n",
    "        \"\"\"Select features based on PCA component loadings\"\"\"\n",
    "        # Fix: Use all components\n",
    "        pca = PCA()\n",
    "        pca.fit(self.scaled_data)\n",
    "        \n",
    "        # Method 1: Sum of absolute loadings across components\n",
    "        absolute_components = np.abs(pca.components_)\n",
    "        feature_importance = absolute_components.sum(axis=0)\n",
    "        \n",
    "        # Method 2: Weighted by explained variance\n",
    "        weighted_importance = np.sum(absolute_components * pca.explained_variance_ratio_[:, np.newaxis], axis=0)\n",
    "        \n",
    "        top_indices = weighted_importance.argsort()[-self.n_features:]\n",
    "        return self.data.columns[top_indices].tolist(), weighted_importance[top_indices]\n",
    "    \n",
    "    def variance_based_selection(self):\n",
    "        \"\"\"Select features with highest variance\"\"\"\n",
    "        variances = self.data.var()\n",
    "        top_features = variances.nlargest(self.n_features)\n",
    "        return top_features.index.tolist(), top_features.values\n",
    "    \n",
    "    def correlation_based_selection(self, threshold=0.9):\n",
    "        \"\"\"Remove highly correlated features\"\"\"\n",
    "        corr_matrix = self.data.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find features with correlation greater than threshold\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "        selected_features = [col for col in self.data.columns if col not in to_drop]\n",
    "        \n",
    "        # If still too many features, combine with variance\n",
    "        if len(selected_features) > self.n_features:\n",
    "            variances = self.data[selected_features].var()\n",
    "            selected_features = variances.nlargest(self.n_features).index.tolist()\n",
    "            \n",
    "        return selected_features, None\n",
    "    \n",
    "    def mutual_information_selection(self):\n",
    "        \"\"\"Select features based on mutual information\"\"\"\n",
    "        # Create a synthetic target for unsupervised learning\n",
    "        # Using first principal component as proxy\n",
    "        pca = PCA(n_components=1)\n",
    "        target = pca.fit_transform(self.scaled_data).ravel()\n",
    "        \n",
    "        mi_scores = mutual_info_regression(self.scaled_data, target, random_state=42)\n",
    "        top_indices = mi_scores.argsort()[-self.n_features:]\n",
    "        \n",
    "        return self.data.columns[top_indices].tolist(), mi_scores[top_indices]\n",
    "    \n",
    "    def random_forest_importance(self):\n",
    "        \"\"\"Use Random Forest feature importance\"\"\"\n",
    "        # Create synthetic target using k-means cluster labels\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "        target = kmeans.fit_predict(self.scaled_data)\n",
    "        \n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(self.scaled_data, target)\n",
    "        \n",
    "        importances = rf.feature_importances_\n",
    "        top_indices = importances.argsort()[-self.n_features:]\n",
    "        \n",
    "        return self.data.columns[top_indices].tolist(), importances[top_indices]\n",
    "    \n",
    "    def lasso_selection(self, alpha=0.01):\n",
    "        \"\"\"Use LASSO regularization for feature selection\"\"\"\n",
    "        # Create synthetic target\n",
    "        pca = PCA(n_components=1)\n",
    "        target = pca.fit_transform(self.scaled_data).ravel()\n",
    "        \n",
    "        lasso = Lasso(alpha=alpha, random_state=42)\n",
    "        lasso.fit(self.scaled_data, target)\n",
    "        \n",
    "        # Get non-zero coefficients\n",
    "        non_zero_indices = np.where(np.abs(lasso.coef_) > 0)[0]\n",
    "        \n",
    "        if len(non_zero_indices) > self.n_features:\n",
    "            # Sort by absolute coefficient value\n",
    "            sorted_indices = non_zero_indices[np.argsort(np.abs(lasso.coef_[non_zero_indices]))[-self.n_features:]]\n",
    "            return self.data.columns[sorted_indices].tolist(), np.abs(lasso.coef_[sorted_indices])\n",
    "        else:\n",
    "            return self.data.columns[non_zero_indices].tolist(), np.abs(lasso.coef_[non_zero_indices])\n",
    "    \n",
    "    def silhouette_based_selection(self, n_clusters=3, n_iterations=10):\n",
    "        \"\"\"Evaluate feature subsets using silhouette score\"\"\"\n",
    "        feature_scores = {}\n",
    "        \n",
    "        for i, feature in enumerate(self.data.columns):\n",
    "            scores = []\n",
    "            for _ in range(n_iterations):\n",
    "                # Random subset including current feature\n",
    "                other_features = np.random.choice(\n",
    "                    [j for j in range(len(self.data.columns)) if j != i],\n",
    "                    size=min(self.n_features-1, len(self.data.columns)-1),\n",
    "                    replace=False\n",
    "                )\n",
    "                feature_subset = np.append(other_features, i)\n",
    "                \n",
    "                # Cluster with subset\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "                labels = kmeans.fit_predict(self.scaled_data[:, feature_subset])\n",
    "                \n",
    "                # Calculate silhouette score\n",
    "                if len(np.unique(labels)) > 1:\n",
    "                    score = silhouette_score(self.scaled_data[:, feature_subset], labels)\n",
    "                    scores.append(score)\n",
    "            \n",
    "            feature_scores[feature] = np.mean(scores) if scores else 0\n",
    "        \n",
    "        # Sort by average silhouette score\n",
    "        sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_features = [f[0] for f in sorted_features[:self.n_features]]\n",
    "        top_scores = [f[1] for f in sorted_features[:self.n_features]]\n",
    "        \n",
    "        return top_features, top_scores\n",
    "    \n",
    "    def ensemble_selection(self, methods=['pca', 'variance', 'mutual_info', 'random_forest']):\n",
    "        \"\"\"Combine multiple methods using voting\"\"\"\n",
    "        feature_votes = {}\n",
    "        \n",
    "        method_map = {\n",
    "            'pca': self.pca_based_selection,\n",
    "            'variance': self.variance_based_selection,\n",
    "            'mutual_info': self.mutual_information_selection,\n",
    "            'random_forest': self.random_forest_importance,\n",
    "            'correlation': self.correlation_based_selection,\n",
    "            'lasso': self.lasso_selection\n",
    "        }\n",
    "        \n",
    "        for method in methods:\n",
    "            if method in method_map:\n",
    "                features, _ = method_map[method]()\n",
    "                for i, feature in enumerate(features):\n",
    "                    if feature not in feature_votes:\n",
    "                        feature_votes[feature] = 0\n",
    "                    # Weight by rank (higher rank = more points)\n",
    "                    feature_votes[feature] += len(features) - i\n",
    "        \n",
    "        # Sort by votes\n",
    "        sorted_features = sorted(feature_votes.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_features = [f[0] for f in sorted_features[:self.n_features]]\n",
    "        \n",
    "        return top_features\n",
    "    \n",
    "    def visualize_feature_importance(self, method='all'):\n",
    "        \"\"\"Visualize feature importance from different methods\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        methods = {\n",
    "            'PCA-based': self.pca_based_selection,\n",
    "            'Variance': self.variance_based_selection,\n",
    "            'Mutual Information': self.mutual_information_selection,\n",
    "            'Random Forest': self.random_forest_importance,\n",
    "            'LASSO': self.lasso_selection,\n",
    "        }\n",
    "        \n",
    "        for idx, (name, method_func) in enumerate(methods.items()):\n",
    "            features, scores = method_func()\n",
    "            if scores is not None:\n",
    "                axes[idx].barh(range(len(features)), scores)\n",
    "                axes[idx].set_yticks(range(len(features)))\n",
    "                axes[idx].set_yticklabels(features)\n",
    "                axes[idx].set_title(f'{name} Feature Importance')\n",
    "                axes[idx].set_xlabel('Importance Score')\n",
    "        \n",
    "        # Use the last subplot for ensemble results\n",
    "        ensemble_features = self.ensemble_selection()\n",
    "        axes[-1].barh(range(len(ensemble_features)), range(len(ensemble_features), 0, -1))\n",
    "        axes[-1].set_yticks(range(len(ensemble_features)))\n",
    "        axes[-1].set_yticklabels(ensemble_features)\n",
    "        axes[-1].set_title('Ensemble Feature Selection')\n",
    "        axes[-1].set_xlabel('Combined Rank')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfdf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('AppML_InitialProject_test_clustering.csv')\n",
    "\n",
    "# Initialize feature selector\n",
    "selector = ClusteringFeatureSelector(data, n_features=10)\n",
    "\n",
    "# Method 1: PCA-based selection (improved)\n",
    "print(\"PCA-based feature selection:\")\n",
    "features, scores = selector.pca_based_selection()\n",
    "print(features)\n",
    "print()\n",
    "\n",
    "# Method 2: Variance-based selection\n",
    "print(\"Variance-based feature selection:\")\n",
    "features, scores = selector.variance_based_selection()\n",
    "print(features)\n",
    "print()\n",
    "\n",
    "# Method 3: Correlation-based selection\n",
    "print(\"Correlation-based feature selection (removing highly correlated):\")\n",
    "features, _ = selector.correlation_based_selection(threshold=0.9)\n",
    "print(features[:10])\n",
    "print()\n",
    "\n",
    "# Method 4: Mutual Information\n",
    "print(\"Mutual Information feature selection:\")\n",
    "features, scores = selector.mutual_information_selection()\n",
    "print(features)\n",
    "print()\n",
    "\n",
    "# Method 5: Random Forest importance\n",
    "print(\"Random Forest feature importance:\")\n",
    "features, scores = selector.random_forest_importance()\n",
    "print(features)\n",
    "print()\n",
    "\n",
    "# Method 6: LASSO selection\n",
    "print(\"LASSO feature selection:\")\n",
    "features, scores = selector.lasso_selection()\n",
    "print(features)\n",
    "print()\n",
    "\n",
    "# Method 7: Ensemble selection\n",
    "print(\"Ensemble feature selection (combining multiple methods):\")\n",
    "ensemble_features = selector.ensemble_selection()\n",
    "print(ensemble_features)\n",
    "\n",
    "# Visualize all methods\n",
    "selector.visualize_feature_importance()\n",
    "\n",
    "selected_data = data[ensemble_features]\n",
    "scaler = StandardScaler()\n",
    "scaled_selected = scaler.fit_transform(selected_data)\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(scaled_selected)\n",
    "\n",
    "# Evaluate clustering quality\n",
    "silhouette_avg = silhouette_score(scaled_selected, clusters)\n",
    "print(f\"\\nSilhouette Score with selected features: {silhouette_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedClusteringAnalysis:\n",
    "    def __init__(self, data, selected_columns=None):\n",
    "        \"\"\"\n",
    "        Initialize clustering analysis\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Input data\n",
    "        selected_columns : list\n",
    "            List of column names to use for clustering\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.selected_columns = selected_columns if selected_columns else data.columns.tolist()\n",
    "        self.X = data[self.selected_columns].values\n",
    "        \n",
    "        # Standardize the data\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_scaled = self.scaler.fit_transform(self.X)\n",
    "        \n",
    "        # Store dimensionality reduction results\n",
    "        self.dim_reduction_results = {}\n",
    "        \n",
    "    def perform_dimensionality_reduction(self):\n",
    "        \"\"\"Perform PCA, t-SNE, and UMAP\"\"\"\n",
    "        print(\"Performing dimensionality reduction...\")\n",
    "        \n",
    "        # PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        self.dim_reduction_results['PCA'] = pca.fit_transform(self.X_scaled)\n",
    "        print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "        \n",
    "        # t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "        self.dim_reduction_results['t-SNE'] = tsne.fit_transform(self.X_scaled)\n",
    "        \n",
    "        # UMAP\n",
    "        umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "        self.dim_reduction_results['UMAP'] = umap_reducer.fit_transform(self.X_scaled)\n",
    "        \n",
    "        print(\"Dimensionality reduction completed!\")\n",
    "        \n",
    "    def find_optimal_kmeans_clusters(self, max_clusters=15):\n",
    "        \"\"\"Find optimal number of clusters for K-means using multiple methods\"\"\"\n",
    "        print(\"\\nFinding optimal number of clusters for K-means...\")\n",
    "        \n",
    "        inertias = []\n",
    "        silhouette_scores = []\n",
    "        davies_bouldin_scores = []\n",
    "        calinski_scores = []\n",
    "        \n",
    "        K = range(2, max_clusters + 1)\n",
    "        \n",
    "        for k in K:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(self.X_scaled)\n",
    "            \n",
    "            inertias.append(kmeans.inertia_)\n",
    "            silhouette_scores.append(silhouette_score(self.X_scaled, labels))\n",
    "            davies_bouldin_scores.append(davies_bouldin_score(self.X_scaled, labels))\n",
    "            calinski_scores.append(calinski_harabasz_score(self.X_scaled, labels))\n",
    "        \n",
    "        # Plot evaluation metrics\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Elbow method\n",
    "        axes[0, 0].plot(K, inertias, 'bo-')\n",
    "        axes[0, 0].set_xlabel('Number of clusters (k)')\n",
    "        axes[0, 0].set_ylabel('Inertia')\n",
    "        axes[0, 0].set_title('Elbow Method')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Silhouette score\n",
    "        axes[0, 1].plot(K, silhouette_scores, 'ro-')\n",
    "        axes[0, 1].set_xlabel('Number of clusters (k)')\n",
    "        axes[0, 1].set_ylabel('Silhouette Score')\n",
    "        axes[0, 1].set_title('Silhouette Analysis (Higher is better)')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Davies-Bouldin Index\n",
    "        axes[1, 0].plot(K, davies_bouldin_scores, 'go-')\n",
    "        axes[1, 0].set_xlabel('Number of clusters (k)')\n",
    "        axes[1, 0].set_ylabel('Davies-Bouldin Index')\n",
    "        axes[1, 0].set_title('Davies-Bouldin Index (Lower is better)')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Calinski-Harabasz Index\n",
    "        axes[1, 1].plot(K, calinski_scores, 'mo-')\n",
    "        axes[1, 1].set_xlabel('Number of clusters (k)')\n",
    "        axes[1, 1].set_ylabel('Calinski-Harabasz Index')\n",
    "        axes[1, 1].set_title('Calinski-Harabasz Index (Higher is better)')\n",
    "        axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find optimal k\n",
    "        optimal_k_silhouette = K[np.argmax(silhouette_scores)]\n",
    "        optimal_k_davies = K[np.argmin(davies_bouldin_scores)]\n",
    "        \n",
    "        print(f\"Optimal k based on Silhouette Score: {optimal_k_silhouette}\")\n",
    "        print(f\"Optimal k based on Davies-Bouldin Index: {optimal_k_davies}\")\n",
    "        \n",
    "        return optimal_k_silhouette\n",
    "    \n",
    "    def find_optimal_dbscan_params(self):\n",
    "        \"\"\"Find optimal eps parameter for DBSCAN using k-distance graph\"\"\"\n",
    "        print(\"\\nFinding optimal DBSCAN parameters...\")\n",
    "        \n",
    "        # Calculate k-distance graph\n",
    "        neighbors = NearestNeighbors(n_neighbors=5)\n",
    "        neighbors_fit = neighbors.fit(self.X_scaled)\n",
    "        distances, indices = neighbors_fit.kneighbors(self.X_scaled)\n",
    "        \n",
    "        # Sort distances\n",
    "        distances = np.sort(distances[:, -1], axis=0)\n",
    "        \n",
    "        # Plot k-distance graph\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(distances)\n",
    "        plt.xlabel('Points sorted by distance')\n",
    "        plt.ylabel('4th Nearest Neighbor Distance')\n",
    "        plt.title('K-distance Graph for DBSCAN eps Selection')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Try different parameter combinations\n",
    "        eps_values = np.linspace(np.percentile(distances, 90), np.percentile(distances, 99), 10)\n",
    "        min_samples_values = [3, 4, 5, 6]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for eps in eps_values:\n",
    "            for min_samples in min_samples_values:\n",
    "                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                labels = dbscan.fit_predict(self.X_scaled)\n",
    "                \n",
    "                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                n_noise = list(labels).count(-1)\n",
    "                \n",
    "                if n_clusters > 1:  # Only calculate if we have clusters\n",
    "                    # Filter out noise points for silhouette score\n",
    "                    mask = labels != -1\n",
    "                    if np.sum(mask) > 0 and len(set(labels[mask])) > 1:\n",
    "                        sil_score = silhouette_score(self.X_scaled[mask], labels[mask])\n",
    "                    else:\n",
    "                        sil_score = -1\n",
    "                else:\n",
    "                    sil_score = -1\n",
    "                \n",
    "                results.append({\n",
    "                    'eps': eps,\n",
    "                    'min_samples': min_samples,\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'n_noise': n_noise,\n",
    "                    'silhouette': sil_score\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Find best parameters\n",
    "        valid_results = results_df[results_df['silhouette'] > 0]\n",
    "        if not valid_results.empty:\n",
    "            best_params = valid_results.loc[valid_results['silhouette'].idxmax()]\n",
    "            print(f\"\\nBest DBSCAN parameters:\")\n",
    "            print(f\"eps: {best_params['eps']:.3f}\")\n",
    "            print(f\"min_samples: {int(best_params['min_samples'])}\")\n",
    "            print(f\"Number of clusters: {int(best_params['n_clusters'])}\")\n",
    "            print(f\"Noise points: {int(best_params['n_noise'])}\")\n",
    "            print(f\"Silhouette score: {best_params['silhouette']:.3f}\")\n",
    "            \n",
    "            return best_params['eps'], int(best_params['min_samples'])\n",
    "        else:\n",
    "            print(\"No valid DBSCAN parameters found. Using defaults.\")\n",
    "            return 0.5, 5\n",
    "    \n",
    "    def perform_clustering(self, optimal_k=None, eps=None, min_samples=None):\n",
    "        \"\"\"Perform various clustering algorithms\"\"\"\n",
    "        print(\"\\nPerforming clustering...\")\n",
    "        \n",
    "        clustering_results = {}\n",
    "        \n",
    "        # K-means\n",
    "        if optimal_k is None:\n",
    "            optimal_k = self.find_optimal_kmeans_clusters()\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "        clustering_results['K-means'] = kmeans.fit_predict(self.X_scaled)\n",
    "        \n",
    "        # DBSCAN\n",
    "        if eps is None or min_samples is None:\n",
    "            eps, min_samples = self.find_optimal_dbscan_params()\n",
    "        \n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        clustering_results['DBSCAN'] = dbscan.fit_predict(self.X_scaled)\n",
    "        \n",
    "        # Gaussian Mixture Model\n",
    "        gmm = GaussianMixture(n_components=optimal_k, random_state=42, covariance_type='full')\n",
    "        clustering_results['GMM'] = gmm.fit_predict(self.X_scaled)\n",
    "        \n",
    "        # Spectral Clustering\n",
    "        spectral = SpectralClustering(n_clusters=optimal_k, random_state=42, affinity='rbf')\n",
    "        clustering_results['Spectral'] = spectral.fit_predict(self.X_scaled)\n",
    "        \n",
    "        # Hierarchical Clustering\n",
    "        hierarchical = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "        clustering_results['Hierarchical'] = hierarchical.fit_predict(self.X_scaled)\n",
    "        \n",
    "        # OPTICS (similar to DBSCAN but more robust)\n",
    "        optics = OPTICS(min_samples=min_samples, xi=0.05, min_cluster_size=0.05)\n",
    "        clustering_results['OPTICS'] = optics.fit_predict(self.X_scaled)\n",
    "\n",
    "        hdb = hdbscan.HDBSCAN(min_cluster_size=min_samples, min_samples=min_samples)\n",
    "        clustering_results['HDBSCAN'] = hdb.fit_predict(self.X_scaled)\n",
    "        \n",
    "        self.clustering_results = clustering_results\n",
    "        \n",
    "        # Evaluate clustering results\n",
    "        self.evaluate_clustering()\n",
    "        \n",
    "        return clustering_results\n",
    "    \n",
    "    def evaluate_clustering(self):\n",
    "        \"\"\"Evaluate all clustering results\"\"\"\n",
    "        print(\"\\nClustering Evaluation:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        evaluation_results = []\n",
    "        \n",
    "        for name, labels in self.clustering_results.items():\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            n_noise = list(labels).count(-1)\n",
    "            \n",
    "            # Calculate metrics only if we have valid clusters\n",
    "            if n_clusters > 1:\n",
    "                # Filter out noise points for metrics\n",
    "                mask = labels != -1\n",
    "                if np.sum(mask) > 0 and len(set(labels[mask])) > 1:\n",
    "                    sil_score = silhouette_score(self.X_scaled[mask], labels[mask])\n",
    "                    db_score = davies_bouldin_score(self.X_scaled[mask], labels[mask])\n",
    "                    ch_score = calinski_harabasz_score(self.X_scaled[mask], labels[mask])\n",
    "                else:\n",
    "                    sil_score = db_score = ch_score = np.nan\n",
    "            else:\n",
    "                sil_score = db_score = ch_score = np.nan\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                'Algorithm': name,\n",
    "                'Clusters': n_clusters,\n",
    "                'Noise Points': n_noise,\n",
    "                'Silhouette': sil_score,\n",
    "                'Davies-Bouldin': db_score,\n",
    "                'Calinski-Harabasz': ch_score\n",
    "            })\n",
    "            \n",
    "        eval_df = pd.DataFrame(evaluation_results)\n",
    "        print(eval_df.to_string(index=False, float_format='%.3f'))\n",
    "        \n",
    "        return eval_df\n",
    "    \n",
    "    def visualize_all_results(self):\n",
    "        \"\"\"Create comprehensive visualization of all results\"\"\"\n",
    "        dim_methods = ['PCA', 't-SNE', 'UMAP']\n",
    "        cluster_methods = list(self.clustering_results.keys())\n",
    "        \n",
    "        # Create subplots for each combination\n",
    "        n_rows = len(cluster_methods)\n",
    "        n_cols = len(dim_methods)\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, cluster_method in enumerate(cluster_methods):\n",
    "            for j, dim_method in enumerate(dim_methods):\n",
    "                ax = axes[i, j]\n",
    "                \n",
    "                # Get data\n",
    "                X_reduced = self.dim_reduction_results[dim_method]\n",
    "                labels = self.clustering_results[cluster_method]\n",
    "                \n",
    "                # Create scatter plot\n",
    "                scatter = ax.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                                   c=labels, cmap='viridis', s=50, alpha=0.6)\n",
    "                \n",
    "                # Highlight noise points if any\n",
    "                if -1 in labels:\n",
    "                    noise_mask = labels == -1\n",
    "                    ax.scatter(X_reduced[noise_mask, 0], X_reduced[noise_mask, 1], \n",
    "                             c='red', marker='x', s=50, label='Noise')\n",
    "                \n",
    "                ax.set_title(f'{cluster_method} - {dim_method}')\n",
    "                ax.set_xlabel(f'{dim_method} Component 1')\n",
    "                ax.set_ylabel(f'{dim_method} Component 2')\n",
    "                \n",
    "                # Add colorbar\n",
    "                plt.colorbar(scatter, ax=ax)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_cluster_comparison(self):\n",
    "        \"\"\"Plot side-by-side comparison of different clustering algorithms\"\"\"\n",
    "        # Use UMAP for visualization (usually gives best separation)\n",
    "        X_viz = self.dim_reduction_results.get('UMAP', self.dim_reduction_results['PCA'])\n",
    "        \n",
    "        n_algorithms = len(self.clustering_results)\n",
    "        n_cols = 3\n",
    "        n_rows = (n_algorithms + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, (name, labels) in enumerate(self.clustering_results.items()):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            scatter = ax.scatter(X_viz[:, 0], X_viz[:, 1], c=labels, \n",
    "                               cmap='viridis', s=50, alpha=0.6)\n",
    "            \n",
    "            if -1 in labels:\n",
    "                noise_mask = labels == -1\n",
    "                ax.scatter(X_viz[noise_mask, 0], X_viz[noise_mask, 1], \n",
    "                         c='red', marker='x', s=50, label='Noise')\n",
    "            \n",
    "            ax.set_title(f'{name}')\n",
    "            ax.set_xlabel('UMAP 1')\n",
    "            ax.set_ylabel('UMAP 2')\n",
    "            plt.colorbar(scatter, ax=ax)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for idx in range(len(self.clustering_results), len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "data = selected_data\n",
    "\n",
    "analyzer = AdvancedClusteringAnalysis(data) \n",
    "\n",
    "analyzer.perform_dimensionality_reduction()\n",
    "\n",
    "analyzer.perform_clustering()\n",
    "\n",
    "analyzer.visualize_all_results()\n",
    "\n",
    "analyzer.plot_cluster_comparison()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128307ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
